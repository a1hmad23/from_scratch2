{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca311a1-e35a-4dff-b49e-7081abf86ada",
   "metadata": {},
   "source": [
    "# Ridge (L$_{2}$) Regularization\n",
    "To prevent overfitting (and reduce variance), in the cost function, add a penalty on the squares of the coefficients: $$\n",
    "J(\\vec{\\beta}) = \\boldsymbol{MSE}(\\vec{\\beta}) + \\lambda\\sum_{i=1}^m \\beta_{j}^2\n",
    "$$where $\\lambda \\geq 0$ is a hyperparameter that controls the extent of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f7e7-fe05-4ba3-927e-375506259059",
   "metadata": {},
   "source": [
    "The derivative with respect to $\\beta_{0}$ will remain unchanged, so we still have the same result as in simple linear regression (OLS): $\\beta_{0} = \\bar{y} - m \\bar{x}$. Moreover, the first term in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3495c9-3edf-4f82-b632-b343e4f17d27",
   "metadata": {},
   "source": [
    "## 2-Dimensional Case\n",
    "The equation for b ($\\beta_{0}$) will remain the same as in simple linear regression, because the derivative of the second term (with respect to $b$) is $0$. Thus, we have $$\\bar{y} - m \\bar{x} = b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdb268-e645-4f6b-91a2-0b65f4d20264",
   "metadata": {},
   "source": [
    "Moreover, in simple linear regression, by setting $\\frac{\\partial J}{\\partial m} = 0$ , we had this intermediary equation: $$\n",
    "\\,m \\sum_{i=1}^n (x_i-\\bar x)^2\n",
    "= \\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)     \\,\n",
    "$$\n",
    "It is to see that the effect of regularization (the term ) $\\lambda\\sum_{i=1}^m \\beta_{j}^2$ in the cost function will just be the following term added to the left side of this above equation, that is, we will have: $$\n",
    "\\,m \\sum_{i=1}^n (x_i-\\bar x)^2 + \\lambda m\n",
    "= \\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)     \\,\n",
    "$$\n",
    "Thus, we arrive at the closed form solution for Ridge Regression in 2D: $$\n",
    "m\n",
    "= \\frac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}\n",
    "       {\\sum_{i=1}^n (x_i-\\bar x)^2 + \\lambda},\\qquad\n",
    "b=\\bar y - m\\bar x\n",
    "$$\n",
    "\n",
    "Defining $X_{c}$ and $Y_{c}$ as $x_i-\\bar x$ and $y_i-\\bar y$  for $1\\leq i\\leq n$, we have the vectorized result: $$ m = \n",
    "\\boxed{\\frac{\\vec{X_{c}} \\hspace{0.1cm} \\cdot \\hspace{0.1cm} \\vec{Y_{c}}}{\\vec{X_{c}} \\hspace{0.1cm} \\cdot \\hspace{0.1cm} \\vec{X_{c} \\hspace{0.1cm}+\\lambda}}} ,\\qquad\n",
    "\\boxed{\\,b=\\bar y - m\\bar x\\,}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161ad12-6547-4215-857b-1407446c6bce",
   "metadata": {},
   "source": [
    "### Implementation of 2D Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3290aa2-43aa-44fb-a3c1-013a3d13fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, n_informative=1, noise=20, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe9f223-35ac-4aa9-bffa-e1093834a06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1d4e47fd400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180aa57d-cf84-4de2-a6e4-bdce6bbd2833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469d1d77-a380-4b43-94aa-322f1ab7d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "class CustomRidgeRegression:\n",
    "    def __init__(self, alpha:float = 1.0):\n",
    "        self.alpha = alpha\n",
    "        self.m = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X = np.asarray(X_train, dtype=float)\n",
    "        y = np.asarray(y_train, dtype=float)\n",
    "\n",
    "        # input validation \n",
    "        if X.size == 0 or y.size == 0:\n",
    "            raise ValueError(\"X_train and y_train must be non-empty.\")\n",
    "        if X.size != y.size:\n",
    "            raise ValueError(\"X_train and y_train must have the same length.\")\n",
    "        if X.ndim != 1 or y.ndim != 1:\n",
    "            raise ValueError(\"CustomLR supports only 1D inputs.\")\n",
    "        if not np.isfinite(X).all() or not np.isfinite(y).all():\n",
    "            raise ValueError(\"Inputs contain NaN or infinite values.\")\n",
    "\n",
    "        # compute slope and intercept \n",
    "        self.x_bar, self.y_bar = X.mean(), y.mean()\n",
    "        X_c, y_c = X - self.x_bar, y - self.y_bar\n",
    "\n",
    "        denom = np.dot(X_c, X_c) + self.alpha\n",
    "        if denom == 0.0:\n",
    "            raise ValueError(\"All X values are identical; slope is undefined.\")\n",
    "\n",
    "        self.m = np.dot(X_c, y_c) / denom\n",
    "        self.b = self.y_bar - self.m * self.x_bar\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test: Iterable[float]) -> np.ndarray:\n",
    "        \"\"\"Predict outputs for new 1D input data.\"\"\"\n",
    "        if self.m is None or self.b is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call fit() first.\")\n",
    "\n",
    "        X = np.asarray(X_test, dtype=float)\n",
    "        if X.ndim != 1:\n",
    "            raise ValueError(\"CustomLR supports only 1D inputs.\")\n",
    "        return self.m * X + self.b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a3001a8-df48-459c-bbec-31b2c399610e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomRidgeRegression at 0x1d4e8071310>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ridge = CustomRidgeRegression()\n",
    "custom_ridge.fit(X_train.ravel(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c4984e-b1a0-4506-952e-f64621491fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = custom_ridge.predict(X_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc42e069-4569-434e-9579-d93409c7d637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6360228303695721"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f623f4-62e5-4ef2-b1df-17c414ed95c3",
   "metadata": {},
   "source": [
    "## n-dimensional Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb2b63-afa6-4652-8381-3f9cc3a11ab1",
   "metadata": {},
   "source": [
    "In multiple Linear Regression, we arrived ar the following vectorized equation for the cost function: $$\n",
    "J(\\boldsymbol{\\beta}) = \\boldsymbol{Y^TY}-2\\boldsymbol{Y^TX\\beta} - \\boldsymbol{\\beta ^TX^TX\\beta}\n",
    "$$\n",
    "For n-dimensional Ridge Regression, we want to add the term $\\lambda\\sum_{i=1}^m \\beta_{j}^2$ to the above cost function. Notice that the intercept is not penalized (because only slopes matter for regularization, so it does not help prevent overfitting. moreover, it breaks translation invariance.) Thus, in vectorized form, we cannot simply add the term $\\vec{\\beta^T} \\cdot \\vec{\\beta}$ . Hence, we add the following term to the cost function:\n",
    "$$\\vec{\\beta}\\Lambda \\beta$$\n",
    "where $\\Lambda = diag(0, 1, \\dots, 1)$\n",
    "Thus, we arrive at the vectorized form of the cost function for n-dimensional Ridge Regression:\n",
    "$$\\boxed{\\,J(\\boldsymbol{\\beta}) = \\boldsymbol{Y^TY}-2\\boldsymbol{Y^TX\\beta} - \\boldsymbol{\\beta ^TX^TX\\beta}\n",
    "+ \\lambda\\vec{\\beta}\\Lambda \\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa562f89-ac83-4f53-9fb0-2c36de78e616",
   "metadata": {},
   "source": [
    "In multiple linear regression, we had found the following intermediary equation:  $$\n",
    " \\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = 0 - 2\\boldsymbol{Y^TX} - 2 \\boldsymbol{\\beta^TX^T X}\n",
    "$$ It is clear that for our case we just have to add the derivative of the regularization term to the above equation:\n",
    "\n",
    "$$\n",
    "R = \\lambda\\, \\beta^T \\Lambda \\beta.\n",
    "$$\n",
    "\n",
    "We use the identity:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} (\\beta^T A \\beta) = (A + A^T)\\beta.\n",
    "$$\n",
    "\n",
    "Since $\\Lambda$ is diagonal, it is symmetric:\n",
    "\n",
    "$$\n",
    "\\Lambda^T = \\Lambda.\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta R\n",
    "= \\lambda (\\Lambda + \\Lambda^T)\\beta\n",
    "= \\lambda (2\\Lambda)\\beta\n",
    "= 2\\lambda\\,\\Lambda\\beta.\n",
    "$$\n",
    "\n",
    "Thus, we arrive at:  $$\n",
    " \\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = 0 - 2\\boldsymbol{Y^TX} - 2 \\boldsymbol{\\beta^TX^T X} + 2\\lambda\\,\\Lambda\\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23970d43-9b7b-4dc7-97f0-15edc1662b96",
   "metadata": {},
   "source": [
    "Using the identity \n",
    "$$\n",
    "\\frac{\\partial (\\beta^T A \\beta)}{\\partial \\beta} = (A + A^T)\\beta = 2A\\beta,\n",
    "$$\n",
    "which applies since $\\boldsymbol{A} = \\boldsymbol{X^T X}$ and $\\Lambda$ are symmetric, we obtain:\n",
    "\n",
    "$$\n",
    "-2\\boldsymbol{Y^T X} - 2\\boldsymbol{\\beta^T X^T X} + 2\\lambda\\,\\Lambda \\boldsymbol{\\beta} = 0.\n",
    "$$\n",
    "\n",
    "divide by $2$ and rearrange:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Y^T X} = \\boldsymbol{\\beta^T X^T X} - \\lambda\\, \\boldsymbol{\\beta^T \\Lambda}.\n",
    "$$\n",
    "\n",
    "transpose both sides:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^T Y} = (X^T X)\\boldsymbol{\\beta} + \\lambda\\,\\Lambda \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "factor out $\\boldsymbol{\\beta}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^T Y} = \\bigl(X^T X + \\lambda \\Lambda\\bigr)\\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "solving for $\\boldsymbol{\\beta}$ gives us the closed-form solution for n-dimensional Ridge regression:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\boldsymbol{\\beta} \n",
    "= \\bigl(X^T X + \\lambda \\Lambda\\bigr)^{-1} X^T Y\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81715bc-0b2b-415f-a6d9-767e4b24a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCustomRidgeRegression:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        self.alpha = alpha\n",
    "        self.beta = None           \n",
    "        self.coef_ = None          \n",
    "        self.intercept_ = None     \n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X = np.asarray(X_train, dtype=float)\n",
    "        y = np.asarray(y_train, dtype=float)\n",
    "\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError(\"y_train must be a 1D array.\")\n",
    "\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X_train and y_train must have the same number of rows.\")\n",
    "\n",
    "        if not np.isfinite(X).all() or not np.isfinite(y).all():\n",
    "            raise ValueError(\"Inputs contain NaN or infinite values.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        X_design = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "        # The first entry is 0 so we don't regularize the intercept.\n",
    "        Lambda = np.eye(n_features + 1)\n",
    "        Lambda[0, 0] = 0\n",
    "\n",
    "        # Compute the ridge-regularized normal equation pieces\n",
    "        XtX = X_design.T @ X_design\n",
    "        Xty = X_design.T @ y\n",
    "\n",
    "        # Solve for beta using the closed-form ridge regression formula\n",
    "        A = XtX + self.alpha * Lambda\n",
    "        self.beta = np.linalg.inv(A) @ Xty\n",
    "\n",
    "        self.intercept_ = self.beta[0]\n",
    "        self.coef_ = self.beta[1:]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.beta is None:\n",
    "            raise ValueError(\"Model is not fitted yet—call fit() first.\")\n",
    "\n",
    "        X = np.asarray(X_test, dtype=float)\n",
    "\n",
    "        # Allow 1D input for convenience\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        return self.intercept_ + X @ self.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af378010-9f3a-4607-9015-6892562e0bb4",
   "metadata": {},
   "source": [
    "# Gradient Descent for N-Dimensional Ridge Regression\n",
    "Using the cost function above, we apply gradient descent by iterating the following updates: $$\n",
    "\\vec{\\beta}=\\vec{\\beta} - \\alpha\\frac{\\partial J}{\\partial\\vec{\\beta}}\n",
    "$$\n",
    "above, we had found the gradient: $$\n",
    "-2\\boldsymbol{Y^T X} - 2\\boldsymbol{\\beta^T X^T X} + 2\\lambda\\,\\Lambda \\boldsymbol{\\beta} = \\frac{\\partial J}{\\partial\\vec{\\beta}}\n",
    "$$\n",
    "Thus, the update rule becomes:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\vec{\\beta}\n",
    "\\leftarrow \n",
    "\\vec{\\beta}\n",
    "- 2\\alpha\\left( \n",
    "(X^T X + \\lambda \\Lambda)\\,\\vec{\\beta}\n",
    "- X^T Y\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0e875fe-db64-43da-84a4-756389d12630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRidgeGD:\n",
    "    def __init__(self, alpha=1.0, lr=0.001, n_iters=1000):\n",
    "        self.alpha = alpha        # regularization strength λ\n",
    "        self.lr = lr              # learning rate η\n",
    "        self.n_iters = n_iters    # number of gradient descent steps\n",
    "        self.beta = None          # full parameter vector (intercept + weights)\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X = np.asarray(X_train, dtype=float)\n",
    "        y = np.asarray(y_train, dtype=float)\n",
    "\n",
    "        # Turn 1D input into 2D\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if y.ndim != 1:\n",
    "            raise ValueError(\"y_train must be a 1D vector.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Add the intercept column\n",
    "        X_design = np.column_stack([np.ones(n_samples), X])\n",
    "\n",
    "        # Ridge penalty matrix (no penalty on intercept)\n",
    "        Lambda = np.eye(n_features + 1)\n",
    "        Lambda[0, 0] = 0\n",
    "\n",
    "        # Initialize \n",
    "        self.beta = np.zeros(n_features + 1)\n",
    "\n",
    "        XtX = X_design.T @ X_design\n",
    "        Xty = X_design.T @ y\n",
    "\n",
    "        # Gradient descent iterations\n",
    "        for _ in range(self.n_iters):\n",
    "            # Gradient:\n",
    "            grad = 2 * ((XtX + self.alpha * Lambda) @ self.beta - Xty)\n",
    "\n",
    "            # Update beta\n",
    "            self.beta -= self.lr * grad\n",
    "\n",
    "        # Store intercept and slope coefficients separately\n",
    "        self.intercept_ = self.beta[0]\n",
    "        self.coef_ = self.beta[1:]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.beta is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call fit() first.\")\n",
    "\n",
    "        X = np.asarray(X_test, dtype=float)\n",
    "\n",
    "        # Allow simple 1D input\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        return self.intercept_ + X @ self.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617f3d4-b9a4-4779-a34b-0f52cb347406",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "The Lasso cost function is:\n",
    "\n",
    "$$\n",
    "J(\\beta) = MSE(\\beta) + \\lambda \\sum_{i=1}^{p} |\\beta_i|.\n",
    "$$\n",
    "\n",
    "In vectorized form, this is\n",
    "$$\n",
    "J(\\beta) = (Y - X\\beta)^{T}(Y - X\\beta) + \\lambda \\|\\beta\\|_{1}.\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\|\\beta\\|_{1} = \\sum_{i=1}^{p} |\\beta_i|.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571831e9-2911-4ec8-b165-0a4d1804e7bc",
   "metadata": {},
   "source": [
    "While in Ridge the penalty term was: $\\lambda \\beta^{2}$ and its derivative was $\\frac{d}{d\\beta}\\left( \\lambda \\beta^{2} \\right)= 2\\lambda \\beta$ which is proportional to $\\beta$, hence could only equal 0 if $\\beta$ itself was 0. Moreover, this had the effect that bigger coefficients would shrink more. However, in Lasso, we have the derivative of the penalty:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\beta}\\left( \\lambda |\\beta| \\right)\n",
    "=\n",
    "\\begin{cases}\n",
    "\\lambda, & \\beta > 0 \\\\\n",
    "-\\lambda, & \\beta < 0 \\\\\n",
    "[-\\lambda, \\lambda], & \\beta = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For all nonzero $\\beta$, the shrinkage force is constant, not proportional. This means:\n",
    "\n",
    "- Big coefficients and small coefficients are shrunk by the same amount, not proportionally.  \n",
    "- Lasso applies equal pressure to push all parameters toward zero.  \n",
    "- That pressure does not get stronger with larger $\\beta$ (unlike ridge).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35dde00-284f-4d36-a39b-e42490627560",
   "metadata": {},
   "source": [
    "## Sparsity in Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de3301-5530-4acd-9599-5b3f402f5034",
   "metadata": {},
   "source": [
    "From before, we have the derivative of the MSE:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\beta}(A\\beta^{2} - 2B\\beta)\n",
    "= 2A\\beta - 2B.\n",
    "$$\n",
    "\n",
    "For the Lasso part:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\beta}(\\lambda|\\beta|)\n",
    "=\n",
    "\\begin{cases}\n",
    "\\lambda, & \\beta > 0 \\\\\n",
    "-\\lambda, & \\beta < 0 \\\\\n",
    "[-\\lambda, \\lambda], & \\beta = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    " Case 1: $( \\beta > 0 \\)$\n",
    "\n",
    "$$\n",
    "2A\\beta - 2B + \\lambda = 0\n",
    "$$\n",
    "\n",
    "Solve for \\( \\beta \\):\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{B - \\lambda/2}{A}.\n",
    "$$\n",
    "\n",
    "This solution only makes sense if:\n",
    "\n",
    "$$\n",
    "B > \\frac{\\lambda}{2}.\n",
    "$$\n",
    "\n",
    "\n",
    "Case 2: $( \\beta < 0 \\)$\n",
    "\n",
    "$$\n",
    "2A\\beta - 2B - \\lambda = 0\n",
    "$$\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\n",
    "\\beta = \\frac{B + \\lambda/2}{A}.\n",
    "$$\n",
    "\n",
    "This solution only holds if:\n",
    "\n",
    "$$\n",
    "B < -\\frac{\\lambda}{2}.\n",
    "$$\n",
    "\n",
    "\n",
    "Case 3: $( \\beta = 0 \\)$\n",
    "\n",
    "At zero, the subgradient is the interval:\n",
    "\n",
    "$$\n",
    "[-\\lambda, \\lambda].\n",
    "$$\n",
    "\n",
    "We get a valid optimum at \\( \\beta = 0 \\) if:\n",
    "\n",
    "$$\n",
    "-\\lambda \\le 2B \\le \\lambda\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "|B| \\le \\frac{\\lambda}{2}.\n",
    "$$\n",
    "\n",
    "\n",
    "Putting all three cases together:\n",
    "\n",
    "$$\n",
    "\\beta^\\ast =\n",
    "\\begin{cases}\n",
    "\\dfrac{B - \\lambda/2}{A}, & B > \\lambda/2 \\\\[8pt]\n",
    "0, & |B| \\le \\lambda/2 \\\\[8pt]\n",
    "\\dfrac{B + \\lambda/2}{A}, & B < -\\lambda/2\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55659060-6c5e-4829-9c4a-5145dd7d367d",
   "metadata": {},
   "source": [
    "**The above solution shows why Lasso creates sparsity:**\n",
    "\n",
    "**There is a whole region, $( |B| \\le \\lambda/2 )$, where the optimal coefficient is exactly 0.**\n",
    "**Therefore, Lasso naturally performs feature selection by setting many coefficients to 0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35355f-a6af-4af6-ae6a-04d8d36b448c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (from_scratch2)",
   "language": "python",
   "name": "from_scratch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
